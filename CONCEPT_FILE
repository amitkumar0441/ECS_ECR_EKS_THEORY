A) For containerization IN AWS, we have ecs :- Elastic container service. - Main benefit ecs provides autoscaling and autohealing of containers. 
In ECS, we have to option to run the containers :- fargate(serverless architecture) and ec2 instances. 
Main components of ecs :- CLUSTER - TASK DEFINITION , TASK. 
ECR- for pulling images for ecs. ECR- REPOSITORY- IMAGE
In order to expose ecs we have two options :- use public ip address or go with application loadbalancer. 
ECR :- Elastic container registry :- By default in ecr we get private repository 
benefit of ecr :- easy and better integeration with other aws services. 
Most import feature :- SCAN ON PUSH. 
In docker , there is no capability of autohealing and autoscaling.

# Login to ECR (replace <region> and <account-id> with your actual values)
$ aws ecr get-login-password --region <region> | docker login --username AWS --password-stdin <account-id>.dkr.ecr.<region>.amazonaws.com

# Build the Docker image (replace <repo-name> with your ECR repository name)
$ docker build -t <account-id>.dkr.ecr.<region>.amazonaws.com/<repo-name>:latest .

# Push the Docker image to ECR (replace <repo-name> with your ECR repository name)
$ docker push <account-id>.dkr.ecr.<region>.amazonaws.com/<repo-name>:latest
 

B) For kubernetes , we have 4 methods :-
EKS/AKS/GKS - MINIKUBE - KIND - KUBEADM 

C) EKS :- ELASTIC KUBERNETES SERVICES.
Why should eks use for production level ?
Because self managed kubernete is tedious task and error prone. 
Elastic kubernetes service is a managed kubernetes service of aws.
cluster- fargate profile (namespace)
Architecture :- Control plane and Worker node
Important components of control plane :- api server, scheduler, etcd and controller manager
In EKS- we configure only control plane and for worker node we can use fargate and ec2 instances.
Fargate :- serverless compute- will be managed by aws
Ingress :- for exposing pods for outside world, it routes traffic inside cluster, But single ingress will not work , we have to also configure ingress controller . 
Ingress controller is an ALB controller which manage request from outside world and send to ingress resource.
For an Ingress Controller, you have several options depending on your Kubernetes setup and requirements:-
a) NGINX Ingress Controller 
b) AWS ALB Ingress Controller (For AWS)

steps :- 
* install kubectl and eksctl on ubuntu system.:- eksctl CLI is used to work with EKS clusters.

* install aws cli.- aws configure
* create cluster through eksctl but used different namespace :- 

--------------------------------eksctl create cluster --name my-cluster --region region-code --fargate 
                                eksctl create cluster --name my-cluster --region <region-code> --without-nodegroup
                                eksctl delete cluster --name my-cluster --region region-code 

* Node groups contains ec2 instances and fargate details. NODE GROUP- COMPUTE SECTION- NODE GROUPS. 
* ARGOCD requires 2 vcpus and 4gb ram so go with t3.medium. 
* ARGOCD should be installed on eks cluster. not on ec2 instances. 
* when we install argocd, we have to create seperate namespace for argocd. 


*[OpenID Connect provider URL]:- authentication endpoint that allows EKS to securely integrate with AWS IAM. It enables Kubernetes pods to assume IAM roles without using static credentials

* download kubeconfig file :- so that we can run kubectl command for eks cluster:- 
-------------------------------aws eks update-kubeconfig --region ap-south-1 --name mycustomclusterfor

* create fargate profile for the application :- so that pods created inside fargate profile alb-sample-app and inside game-2048 namespace. 
eksctl create fargateprofile \
    --cluster mycustomclusterfor \
    --region ap-south-1 \
    --name alb-sample-app \
    --namespace game-2048

* check it on :- eks cluster- compute tab - fargate profile. 

* check config-context so that you can on which platform will my kubectl apply -f command is going to be executed on minikube or eks by using this :- 
----------kubectl config current-context

* --------kubectl apply -f deploymentfile.yml                       :- defined about deployment, service and ingress.
----------kubectl get ingress -n game-2048  - but right now ingress doesn't have address of ingress controller 
----------kubectl get services -n game-2048
----------kubectl get deployments -n game-2048
----------kubectl get pods -n game-2048

* Create  AWS ALB ingress-controller for ingress resource :- 

1) eksctl utils associate-iam-oidc-provider --cluster mycustomclusterfor --approve     - will create odic identity providers so that it can communicate with aws resources

2) curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.11.0/docs/install/iam_policy.json :- download iam policy.

3) create iam policy :- 
aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --policy-document file://iam_policy.json

4) create iam role :- 
eksctl create iamserviceaccount \
  --cluster=<your-cluster-name> \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --role-name AmazonEKSLoadBalancerControllerRole \
  --attach-policy-arn=arn:aws:iam::<your-aws-account-id>:policy/AWSLoadBalancerControllerIAMPolicy \
  --approve

5) helm repo add eks https://aws.github.io/eks-charts 

6) helm repo update eks

7) 
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=<clustername> \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=<regionname> \
  --set vpcId=<vpcid>

8) kubectl get deployments -n kube-system aws-load-balancer-controller 




* INSTALL ARGOCD ON EKS :-
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'
export ARGOCD_SERVER=$(kubectl get svc argocd-server -n argocd -o jsonpath="{.status.loadBalancer.ingress[0].hostname}")  - will store loadbalancer value and argocd logi link to ARGOCD_SERVER variable
export ARGO_PWD=$(kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath="{.data.password}" | base64 --decode) - will store password to ARGO_PWD variable
for login :- use agro dns value :- abeae5fd35220462198e3d7103c8da63-1359925080.ap-south-1.elb.amazonaws.com
user :- admin
password :- dM4ANdn0cAp8tNBf
connect with repo via https 
create new application
path:- foldername in which your manifests file are saved. 

